{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjynzqOM1rlDzrc2ay92eM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meenarekha/GEN-AI/blob/main/story_telling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeZg8F9xT_QY",
        "outputId": "14b27270-7478-48d1-96b5-549e12846619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a prompt for the story: king with is princess\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Story:\n",
            "\n",
            "king with is princesses.\n",
            "\n",
            "\"I'm not sure how to deal with a princess, but I can see that you're quite fond of me,\" the princess said. \"I thought you were a little jealous of the way you looked at my face. I think you'll just be very glad that I'm able to have your kind of attention.\"\n",
            ". . .\n",
            ": \"Oh, yeah. So, you and I have a lot of things going on in our lives. You know, we've been through a few things together, and it's kind and nice to get back to you. We've had a couple of really good times together. And you know how I feel about it, too?\"\n",
            "—\"My sister-in-law is a bit of a mess.\"—The Princesses of Fairytale\n",
            " (2)\n",
            "There was a very specific, dark scene between the two of them. At the very beginning of \"My Little Pony: Friendship is Magic\", the Princess of Friendship looks on with pity and sadness. She seems to be in denial about everything, as if she has forgotten her past. It's not that she's embarrassed, she just seems so much more upset than she is at this moment. But it seems that the point of this scene is to remind her of her sister, that her parents were all in love. In that scene, the scene where Princess Celestia talks about being a \"little princess\", it was really very clear that\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"  # You can also use \"gpt2-medium\", \"gpt2-large\", etc. for better quality\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate a story from a prompt\n",
        "def generate_story(prompt, max_length=300):\n",
        "    # Encode the prompt text\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate a story continuation based on the prompt\n",
        "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7, top_k=50, top_p=0.9, do_sample=True)\n",
        "\n",
        "    # Decode and return the generated story\n",
        "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return story\n",
        "\n",
        "# Get user input (prompt)\n",
        "user_prompt = input(\"Enter a prompt for the story: \")\n",
        "\n",
        "# Generate and display the story\n",
        "generated_story = generate_story(user_prompt)\n",
        "print(\"\\nGenerated Story:\\n\")\n",
        "print(generated_story)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Function to format the story into readable chunks\n",
        "def format_story(text, max_chars=250):\n",
        "    sentences = text.split('.')\n",
        "    story_paragraphs = []\n",
        "    paragraph = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip() == \"\":\n",
        "            continue\n",
        "        if len(paragraph) + len(sentence) < max_chars:\n",
        "            paragraph += sentence.strip() + \". \"\n",
        "        else:\n",
        "            story_paragraphs.append(paragraph.strip())\n",
        "            paragraph = sentence.strip() + \". \"\n",
        "    if paragraph:\n",
        "        story_paragraphs.append(paragraph.strip())\n",
        "\n",
        "    return \"\\n\\n\".join(story_paragraphs)\n",
        "\n",
        "# Function to generate a story from a prompt\n",
        "def generate_story(prompt, max_length=300):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        do_sample=True\n",
        "    )\n",
        "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return format_story(story)\n",
        "\n",
        "# Get user input (prompt)\n",
        "user_prompt = input(\"Enter a prompt for the story: \")\n",
        "\n",
        "# Generate and display the formatted story\n",
        "print(\"\\n--- Generated Story ---\\n\")\n",
        "print(generate_story(user_prompt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSVVAM6-Wtd3",
        "outputId": "f45959a2-494e-4ec9-d2bf-125a6cca3f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a prompt for the story: once upon a time in a distant land\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generated Story ---\n",
            "\n",
            "once upon a time in a distant land, with a great and glorious sky, and the mighty, glorious sea, as the most glorious and mighty of all worlds.\n",
            "\n",
            "For the great, mighty and wonderful world is the greatest of worlds, the largest and greatest, both in number and in size.\n",
            "\n",
            "The most great of the worlds is known as God, of which we have only one, but the rest are called God's sons, because they are the sons of God. So the Lord God is in all things, even in his power.\n",
            "\n",
            "And if he were not in His power, what would be the glory of his Son? For in the beginning there was the Father, that was in heaven and on earth; and He was with the Holy Spirit.\n",
            "\n",
            "But now He is with us, in whom all the world has been made, having the power and glory, so that the whole world might become full of grace. 6.\n",
            "\n",
            "Now when the people heard that they were gathered together to worship Him, they worshipped Him as one body of Christ, their own, without any other, which is not of Him.\n",
            "\n",
            "They worshipped His body as a body, though they worshiped the body and of one Christ. As for the other Christ which was not God before the death of His own body was destroyed, since he was a remnant of sin.\n",
            "\n",
            "Therefore, on the day when He came to be, He did not destroy the flesh, nor did He destroy all flesh. He also did all that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1IdUDo4XPo5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}